---
title: Azure AKS Deployment | Official Documentation
description: Configure deployment with Azure Kubernetes Service (AKS) using OpenMetadata-recommended Helm charts and scalable configuration templates.
sidebarTitle: Aks
collate: false
---

import Faqs from '/snippets/deployment/faqs.mdx'

# OpenMetadata Deployment on Azure Kubernetes Service Cluster

OpenMetadata can be deployed on Azure Kubernetes Service. This guide covers both the recommended Kubernetes orchestrator (new in 1.12) and the alternative Airflow-based orchestrator.

## Prerequisites

### Azure Services for Database and Search Engine as Elastic Cloud

It is recommended to use [Azure SQL](https://azure.microsoft.com/en-in/products/azure-sql/database) and [Elastic Cloud on Azure](https://www.elastic.co/partners/microsoft-azure) for Production Deployments.

We support:
- Azure SQL (MySQL) engine version 8 or higher
- Azure SQL (PostgreSQL) engine version 12 or higher
- Elastic Cloud (ElasticSearch version 8.11.4)

We recommend:
- Azure SQL to be Multi Zone Available and Production Workload Environment
- Elastic Cloud Environment with multiple zones and minimum 2 nodes

### Step 1 - Create an AKS Cluster

If you are deploying on a new cluster set the `EnableAzureDiskFileCSIDriver=true` to enable container storage interface storage drivers.

```azure-cli
az aks create   --resource-group  MyResourceGroup    \
                --name MyAKSClusterName              \
                --nodepool-name agentpool            \
                --outbound-type loadbalancer         \
                --location YourPreferredLocation     \
                --generate-ssh-keys                  \
                --enable-addons monitoring           \
                EnableAzureDiskFileCSIDriver=true
```

For existing cluster it is important to enable the CSI storage drivers:

```azure-cli
az aks update -n MyAKSCluster -g MyResourceGroup --enable-disk-driver --enable-file-driver
```

### Step 2 - Create a Namespace (optional)

```azure-cli
kubectl create namespace openmetadata
```

### Step 3 - Add the Helm OpenMetadata Repo

```azure-cli
helm repo add open-metadata https://helm.open-metadata.org/
helm repo update
```

---

## Kubernetes Orchestrator Configuration (Recommended)

Starting with OpenMetadata 1.12, we recommend using the **Kubernetes native orchestrator** for running ingestion pipelines. This eliminates the need for Apache Airflow and simplifies your deployment.

<Tip>
The Kubernetes orchestrator runs ingestion pipelines as native K8s Jobs and CronJobs. For full documentation on features, configuration options, and troubleshooting, see the [Kubernetes Orchestrator Guide](/deployment/ingestion/kubernetes).
</Tip>

<Warning>
The recommended OMJob Operator approach requires installing Custom Resource Definitions (CRDs), which needs elevated cluster permissions. If your cluster policies don't allow CRDs, you can disable the operator by setting `useOMJobOperator: false` and `omjobOperator.enabled: false` in your values file to use native K8s Jobs instead.
</Warning>

### Create Kubernetes Secrets

Create the required secrets for your database and search engine:

```azure-cli
# Database secret (for MySQL)
kubectl create secret generic mysql-secrets \
  --namespace openmetadata \
  --from-literal=openmetadata-mysql-password=<YOUR_AZURE_SQL_PASSWORD>

# ElasticSearch secret
kubectl create secret generic elasticsearch-secrets \
  --namespace openmetadata \
  --from-literal=openmetadata-elasticsearch-password=<YOUR_ELASTIC_CLOUD_PASSWORD>
```

### OpenMetadata Values Configuration

Create your `openmetadata-values.yaml` with the following configuration:

```yaml
# openmetadata-values.yaml
openmetadata:
  config:
    # Database configuration
    elasticsearch:
      host: <ELASTIC_CLOUD_ENDPOINT_WITHOUT_HTTPS>
      searchType: elasticsearch
      port: 443
      scheme: https
      connectionTimeoutSecs: 5
      socketTimeoutSecs: 60
      keepAliveTimeoutSecs: 600
      batchSize: 10
      auth:
        enabled: true
        username: <ELASTIC_CLOUD_USERNAME>
        password:
          secretRef: elasticsearch-secrets
          secretKey: openmetadata-elasticsearch-password
    database:
      host: <AZURE_SQL_ENDPOINT>
      port: 3306
      driverClass: com.mysql.cj.jdbc.Driver
      dbScheme: mysql
      dbUseSSL: true
      databaseName: <AZURE_SQL_DATABASE_NAME>
      auth:
        username: <AZURE_SQL_DATABASE_USERNAME>
        password:
          secretRef: mysql-secrets
          secretKey: openmetadata-mysql-password

    # Kubernetes Orchestrator configuration
    pipelineServiceClientConfig:
      enabled: true
      type: "k8s"
      metadataApiEndpoint: http://openmetadata.openmetadata.svc.cluster.local:8585/api

      k8s:
        className: "org.openmetadata.service.clients.pipeline.k8s.K8sPipelineClient"
        ingestionImage: "docker.getcollate.io/openmetadata/ingestion-base:1.12.0"
        imagePullPolicy: "IfNotPresent"
        useOMJobOperator: true

# Enable the OMJob Operator (recommended for production)
omjobOperator:
  enabled: true
  image:
    repository: docker.getcollate.io/openmetadata/omjob-operator
    tag: "1.12.0"

image:
  tag: "1.12.0"
```

<Info>
For advanced configuration options such as resource limits, job lifecycle settings, failure diagnostics, RBAC, and security contexts, see the [Kubernetes Orchestrator Guide](/deployment/ingestion/kubernetes).
</Info>

<Tip>

For Database as PostgreSQL, use the below config for database values:

```yaml
database:
  host: <AZURE_SQL_ENDPOINT>
  port: 5432
  driverClass: org.postgresql.Driver
  dbScheme: postgresql
  dbUseSSL: true
  databaseName: <AZURE_SQL_DATABASE_NAME>
  auth:
    username: <AZURE_SQL_DATABASE_USERNAME>
    password:
      secretRef: postgresql-secret
      secretKey: postgresql-password
```

</Tip>

### Deploy OpenMetadata

```bash
# Install OpenMetadata (no dependencies chart needed with K8s orchestrator)
helm install openmetadata open-metadata/openmetadata \
  --namespace openmetadata \
  --values openmetadata-values.yaml
```

<Tip>
With the Kubernetes orchestrator, you don't need to deploy the `openmetadata-dependencies` chart that includes Airflow. This significantly simplifies your deployment.
</Tip>

### Verify the Deployment

```bash
# Check pods are running
kubectl get pods -n openmetadata

# Check the K8s orchestrator health in OpenMetadata UI
# Navigate to Settings → Preferences → Health
```

### Access OpenMetadata

```azure-cli
kubectl port-forward service/openmetadata 8585:8585 -n openmetadata
```

---

## Using Airflow Orchestrator (Alternative)

If you prefer to use Apache Airflow as the orchestrator (e.g., for existing Airflow investments or complex DAG requirements), follow the configuration below.

<Warning>
Using Airflow requires additional infrastructure: persistent volumes with ReadWriteMany access, the openmetadata-dependencies Helm chart, and more complex configuration.
</Warning>

### Create Persistent Volumes

OpenMetadata helm chart depends on Airflow and Airflow expects a persistent disk that support ReadWriteMany (the volume can be mounted as read-write by many nodes). The Azure CSI storage drivers we enabled earlier support the provisioning of the disks in ReadWriteMany mode.

```yaml
# logs_dags_pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: openmetadata-dependencies-dags-pvc
  namespace: openmetadata
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 10Gi
  storageClassName: azurefile-csi
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: openmetadata-dependencies-logs-pvc
  namespace: openmetadata
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 5Gi
  storageClassName: azurefile-csi
```

Create the volume claims by applying the manifest:

```azure-cli
kubectl apply -f logs_dags_pvc.yaml
```

### Change Owner and Update Permission for Persistent Volumes

Airflow pods run as non-root user and lack write access to our persistent volumes. To fix this we create a job permissions_pod.yaml that runs a pod that mounts volumes into the persistent volume claim and updates the owner of the mounted folders /airflow-dags and /airflow-logs to user id 50000, which is the default linux user id of Airflow pods.

```yaml
# permissions_pod.yaml
apiVersion: batch/v1
kind: Job
metadata:
  labels:
    run: my-permission-pod
  name: my-permission-pod
  namespace: openmetadata
spec:
  template:
    spec:
      containers:
      - image: busybox
        name: my-permission-pod
        volumeMounts:
        - name: airflow-dags
          mountPath: /airflow-dags
        - name: airflow-logs
          mountPath: /airflow-logs
        command: ["/bin/sh", "-c", "chown -R 50000 /airflow-dags /airflow-logs", "chmod -R a+rwx /airflow-dags"]
      restartPolicy: Never
      volumes:
      - name: airflow-logs
        persistentVolumeClaim:
          claimName: openmetadata-dependencies-logs-pvc
      - name: airflow-dags
        persistentVolumeClaim:
          claimName: openmetadata-dependencies-dags-pvc
```

Start the job by applying the manifest:

```azure-cli
kubectl apply -f permissions_pod.yaml
```

### Create Airflow Secrets

```azure-cli
kubectl create secret generic airflow-secrets \
  --namespace openmetadata \
  --from-literal=openmetadata-airflow-password=<AdminPassword>
```

For production deployments connecting external postgresql database:

```azure-cli
kubectl create secret generic postgresql-secret \
  --namespace openmetadata \
  --from-literal=postgresql-password=<MyPGDBPassword>
```

### Install OpenMetadata Dependencies

Create `values-dependencies.yaml` to configure Airflow with persistent volumes:

```yaml
# values-dependencies.yaml
airflow:
  airflow:
    extraVolumeMounts:
      - mountPath: /airflow-logs
        name: aks-airflow-logs
      - mountPath: /airflow-dags/dags
        name: aks-airflow-dags
    extraVolumes:
      - name: aks-airflow-logs
        persistentVolumeClaim:
          claimName: openmetadata-dependencies-logs-pvc
      - name: aks-airflow-dags
        persistentVolumeClaim:
          claimName: openmetadata-dependencies-dags-pvc
    config:
      AIRFLOW__OPENMETADATA_AIRFLOW_APIS__DAG_GENERATED_CONFIGS: "/airflow-dags/dags"
  dags:
    path: /airflow-dags/dags
    persistence:
      enabled: false
  logs:
    path: /airflow-logs
    persistence:
      enabled: false
  externalDatabase:
    type: postgres # default mysql
    host: Host_db_address
    database: Airflow_metastore_dbname
    user: db_userName
    port: 5432
    dbUseSSL: true
    passwordSecret: postgresql-secret
    passwordSecretKey: postgresql-password
```

Install the dependencies:

```azure-cli
helm install openmetadata-dependencies open-metadata/openmetadata-dependencies \
  --values values-dependencies.yaml \
  --namespace openmetadata \
  --set mysql.enabled=false
```

It takes a few minutes for all the pods to be correctly set-up and running:

```azure-cli
kubectl get pods -n openmetadata
```

### Install OpenMetadata with Airflow

Create `openmetadata-values.yaml` for Airflow-based deployment:

```yaml
# openmetadata-values.yaml
global:
  pipelineServiceClientConfig:
    apiEndpoint: http://openmetadata-dependencies-web.openmetadata.svc.cluster.local:8080
    metadataApiEndpoint: http://openmetadata.openmetadata.svc.cluster.local:8585/api

openmetadata:
  config:
    elasticsearch:
      host: <ELASTIC_CLOUD_ENDPOINT_WITHOUT_HTTPS>
      searchType: elasticsearch
      port: 443
      scheme: https
      auth:
        enabled: true
        username: <ELASTIC_CLOUD_USERNAME>
        password:
          secretRef: elasticsearch-secrets
          secretKey: openmetadata-elasticsearch-password
    database:
      host: <AZURE_SQL_ENDPOINT>
      port: 5432
      driverClass: org.postgresql.Driver
      dbScheme: postgresql
      databaseName: openmetadata_db
      auth:
        username: <DB_USERNAME>
        password:
          secretRef: postgresql-secret
          secretKey: postgresql-password

image:
  tag: "1.12.0"
```

```azure-cli
helm install openmetadata open-metadata/openmetadata \
  --values openmetadata-values.yaml \
  --namespace openmetadata
```

## Troubleshooting

### Troubleshooting Airflow

### JSONDecodeError: Unterminated string starting

If you are using Airflow with Azure Blob Storage as `PersistentVolume` as explained in [Storage class using blobfuse](https://learn.microsoft.com/en-us/azure/aks/azure-csi-blob-storage-provision?tabs=mount-nfs%2Csecret),
you may encounter the following error after a few days:

```bash
{dagbag.py:346} ERROR - Failed to import: /airflow-dags/dags/...py
json.decoder.JSONDecodeError: Unterminated string starting at: line 1 column 3552
```

Moreover, the Executor pods would actually be using old files. This behaviour is caused by the recommended config by the
mentioned documentation:

```yaml
  - -o allow_other
  - --file-cache-timeout-in-seconds=120
  - --use-attr-cache=true
  - --cancel-list-on-mount-seconds=10  # prevent billing charges on mounting
  - -o attr_timeout=120
  - -o entry_timeout=120
  - -o negative_timeout=120
  - --log-level=LOG_WARNING  # LOG_WARNING, LOG_INFO, LOG_DEBUG
  - --cache-size-mb=1000  # Default will be 80% of available memory, eviction will happen beyond that.
```

**Disabling the cache** will help here. In this case it won't have any negative impact, since the `.py` and `.json`
files are small enough and not heavily used.

The same configuration without cache:

```yaml
  - --o direct_io
  - --file-cache-timeout-in-seconds=0
  - --use-attr-cache=false
  - --cancel-list-on-mount-seconds=10
  - --o attr_timeout=0
  - --o entry_timeout=0
  - --o negative_timeout=0
  - --log-level=LOG_WARNING
  - --cache-size-mb=0
```

You can find more information about this error [here](https://github.com/open-metadata/OpenMetadata/issues/15321), and similar
discussions [here](https://github.com/Azure/azure-storage-fuse/issues/1171) and [here](https://github.com/Azure/azure-storage-fuse/issues/1139).

## FAQs

<Faqs />