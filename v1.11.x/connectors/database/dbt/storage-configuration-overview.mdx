---
title: dbt Artifact Configuration Guide | OpenMetadata
description: Choose the right storage method for your dbt artifacts with OpenMetadata. Compare S3, GCS, Azure, HTTP, and Local storage options.
slug: /connectors/database/dbt/storage-configuration-overview
sidebarTitle: Overview
---

# dbt Artifact Configuration

<Note>
**Using dbt Cloud?** You don't need storage configuration. Go directly to the [dbt Cloud API guide](/connectors/database/dbt/dbt-cloud-api-guide) for a simpler setup.
</Note>

This guide is for **dbt Core** users. When using dbt Core, OpenMetadata needs access to dbt-generated artifacts (manifest.json, catalog.json, run_results.json) to extract metadata, lineage, and test results. Since dbt Core runs within your infrastructure, you must configure a storage method to make these artifacts accessible to OpenMetadata.

## How It Works

If you're using **dbt Core**, you need to set up artifact storage:

1. **Generate artifacts**: Run `dbt run`, `dbt test`, and `dbt docs generate` to create manifest.json, catalog.json, and run_results.json
2. **Upload to storage**: Configure your workflow to upload these files to S3, GCS, Azure, HTTP server, or shared filesystem
3. **Configure OpenMetadata**: Point OpenMetadata to the storage location to pull and process the artifacts

## Storage Options for dbt Core

Choose the storage method that matches your infrastructure:

<CardGroup cols={3}>
  <Card title="AWS S3" href="/connectors/database/dbt/storage-s3-guide" icon="aws">
    Complete S3 setup with Airflow DAG, IAM policies, and configuration
  </Card>

  <Card title="Google Cloud Storage" href="/connectors/database/dbt/storage-gcs-guide" icon="google">
    GCS bucket setup with service accounts and Cloud Composer integration
  </Card>

  <Card title="Azure Blob Storage" href="/connectors/database/dbt/storage-azure-guide" icon="microsoft">
    Azure storage account setup with managed identity and container config
  </Card>

  <Card title="HTTP Server" href="/connectors/database/dbt/storage-http-guide" icon="server">
    Nginx, Apache, or S3+CloudFront configuration for artifact hosting
  </Card>

  <Card title="Local Filesystem" href="/connectors/database/dbt/storage-local-guide" icon="folder">
    Docker volumes, Kubernetes PVC, or NFS shared filesystem setup
  </Card>
</CardGroup>

## Quick Setup Summary

### AWS S3
```bash
# Create bucket
aws s3 mb s3://your-dbt-artifacts

# Upload artifacts (manual)
aws s3 sync target/ s3://your-dbt-artifacts/dbt/ --include "*.json"

# Or use automated Airflow DAG (see detailed guide)
```
[View complete S3 guide →](/connectors/database/dbt/storage-s3-guide)

### Google Cloud Storage
```bash
# Create bucket
gsutil mb gs://your-dbt-artifacts

# Upload artifacts (manual)
gsutil -m cp target/*.json gs://your-dbt-artifacts/dbt/

# Or use Cloud Composer DAG (see detailed guide)
```
[View complete GCS guide →](/connectors/database/dbt/storage-gcs-guide)

### Azure Blob Storage
```bash
# Create storage account and container
az storage account create --name yourdbtartifacts
az storage container create --name dbt-artifacts

# Upload artifacts
az storage blob upload-batch \
  --destination dbt-artifacts \
  --source target/ \
  --pattern "*.json"
```
[View complete Azure guide →](/connectors/database/dbt/storage-azure-guide)

### HTTP Server
```bash
# Upload via rsync to static file server
rsync -avz target/*.json user@server:/var/www/dbt/

# Or configure S3 + CloudFront for HTTPS access
```
[View complete HTTP guide →](/connectors/database/dbt/storage-http-guide)

### Local/Shared Filesystem
```yaml
# Docker Compose volume mount example
volumes:
  - ./dbt/target:/dbt-artifacts

# OpenMetadata reads directly from /dbt-artifacts path
```
[View complete Local guide →](/connectors/database/dbt/storage-local-guide)

## Common Requirements Across All Methods

Regardless of which storage method you choose, you need:

### 1. Required dbt Artifacts

| File | Generated By | Purpose |
|------|--------------|---------|
| `manifest.json` | `dbt run`, `dbt compile`, `dbt build` | **Required** - Model definitions, sources, lineage, tests |
| `catalog.json` | `dbt docs generate` | Recommended - Column names, types, descriptions |
| `run_results.json` | `dbt test`, `dbt run`, `dbt build` | Optional - Test execution results and timing |

### 2. dbt Command Sequence

Run these commands to generate all artifacts:

```bash
dbt run          # Execute models, generates manifest.json
dbt test         # Run tests, updates run_results.json
dbt docs generate # Generate catalog.json with column metadata
```

### 3. OpenMetadata Configuration

After artifacts are accessible, configure OpenMetadata ingestion:

- **UI Method**: See [Configure dbt Workflow](/connectors/database/dbt/configure-dbt-workflow)
- **CLI Method**: See [Run dbt Workflow Externally](/connectors/database/dbt/run-dbt-workflow-externally)
- **Auto-ingest**: See [Auto Ingest dbt Core](/connectors/database/dbt/auto-ingest-dbt-core)

## Troubleshooting Common Issues

| Issue | Possible Cause | Solution |
|-------|----------------|----------|
| **Artifacts not found** | Upload failed or wrong path | Verify artifacts uploaded successfully to correct location |
| **Access denied** | Insufficient permissions | Check IAM policies, service account permissions, or access keys |
| **Stale metadata** | Old artifacts | Ensure upload happens after dbt completes; verify timestamps |
| **Missing lineage** | No compiled_code in manifest | Run `dbt compile` or `dbt docs generate` before upload |
| **Missing column descriptions** | No catalog.json | Ensure `dbt docs generate` runs and catalog.json is uploaded |
| **Test results not showing** | No run_results.json | Ensure `dbt test` runs and run_results.json is uploaded |

For storage-specific troubleshooting, see the individual guides.

## Next Steps

1. **Choose your storage method** using the decision matrix above
2. **Follow the detailed guide** for your chosen method
3. **Configure OpenMetadata ingestion** after artifacts are accessible
4. **Set up scheduling** to keep metadata synchronized

<Note>
Questions? See the main [dbt Overview](/connectors/database/dbt) or [dbt Troubleshooting](/connectors/database/dbt/dbt-troubleshooting) guide.
</Note>
